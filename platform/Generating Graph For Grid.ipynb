{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as urllib2\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time as t\n",
    "import datetime\n",
    "import itertools\n",
    "import os\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First code blocks are focused on just gathering data for a mesh of atlanta. Essentially 4 closest points are gathered\n",
    "4|V| = |E|. Then, the adj matrix (A) of edges to be collected are collected for array of coords are collected and dumped into csv. $A_{ij}$ implies info going from i to j. What edges to use is dependent only on A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INPUTS:\n",
    "    url: a request url\n",
    "OUTPUTS: \n",
    "    the data returned by calling that url\n",
    "\"\"\"\n",
    "def request_data_from_url(url):\n",
    "    req = urllib2.Request(url)\n",
    "    success = False\n",
    "    while success is False:\n",
    "        try: \n",
    "            #open the url\n",
    "            response = urllib2.urlopen(req)\n",
    "            \n",
    "            #200 is the success code for http\n",
    "            if response.getcode() == 200:\n",
    "                success = True\n",
    "        except Exception as e:\n",
    "            #if we didn't get a success, then print the error and wait 5 seconds before trying again\n",
    "            print(e)\n",
    "            t.sleep(5)\n",
    "\n",
    "            print(\"Error for URL %s: %s\" % (url, datetime.datetime.now()))\n",
    "            print(\"Retrying...\")\n",
    "    return response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INPUTS:\n",
    "    api_key: authentication to GMaps that we're allowed to request this data\n",
    "    origin: lat,long of origin\n",
    "    destination: lat,long of destination\n",
    "OUTPUTS\n",
    "    (time_traffic,time,distance,origin_addr,destination_addr)\n",
    "    real time\n",
    "    usual time\n",
    "    distance\n",
    "\"\"\"\n",
    "def scrape_gmaps_data(api_key, origin, destination):\n",
    "    \n",
    "    #we want to scrape the google maps website\n",
    "    site = 'https://maps.googleapis.com/maps/api/'\n",
    "    \n",
    "    #we want to use the distance matrix service\n",
    "    service = 'distancematrix/json?'\n",
    "    \n",
    "    #input origin and destination from the user \n",
    "    locations = f\"origins={origin[0]},{origin[1]}&destinations={destination[0]},{destination[1]}&departure_time=now&\"\n",
    "\n",
    "    #input api key from user\n",
    "    key = 'key=%s' % (api_key)\n",
    "    \n",
    "    #construct request url\n",
    "    request_url = site + service + locations + key\n",
    "    \n",
    "    # get data from api\n",
    "    data = json.loads(request_data_from_url(request_url))\n",
    "    \n",
    "    #extract travel real time from response\n",
    "    realtime = data['rows'][0]['elements'][0]['duration_in_traffic']['value']\n",
    "    \n",
    "    #extract travel avg time from response\n",
    "    time = data['rows'][0]['elements'][0]['duration']['value']\n",
    "    \n",
    "    #extract distance from response\n",
    "    dist = data['rows'][0]['elements'][0]['duration_in_traffic']['value'] \n",
    "    \n",
    "    # origin address\n",
    "    oa = data['origin_addresses'][0]\n",
    "    \n",
    "    # destination address\n",
    "    da = data['destination_addresses'][0]\n",
    "    \n",
    "    return (realtime, time, dist, oa, da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof that this works! And gives us what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248,\n",
       " 243,\n",
       " 248,\n",
       " '788 Research Dr NW, Atlanta, GA 30313, USA',\n",
       " '177 N Ave NW, Atlanta, GA 30313, USA')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"supersecretapikey.txt\",\"r\") as fh:\n",
    "    api_key = fh.readlines()[0]\n",
    "origin = (33.776360,-84.397824) # Campus Center\n",
    "destination = (33.772428,-84.392709) #Bobby Dodd\n",
    "\n",
    "scrape_gmaps_data(api_key, origin, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INPUTS:\n",
    "    adjacency matrix: sparse boolean matrix representing which edges to query for\n",
    "    location_matrix: np.array floating points of size 2x|N| (Longitude, Latitude)\n",
    "    period: how often to query in minutes\n",
    "    number_of_queries: how many times to query\n",
    "    api_key: api key for your gcp account \n",
    "OUTPUTS\n",
    "    Pandas df\n",
    "\"\"\"\n",
    "def graph_query(location_matrix, adjacency_matrix, period, number_of_queries, api_key):\n",
    "    # basic checks \n",
    "    if location_matrix.shape[1] != adjacency_matrix.shape[0]:\n",
    "        raise ValueError(\"length of locations is not the same as size of adjacency matrix\")\n",
    "    if adjacency_matrix.shape[0] != adjacency_matrix.shape[1]:\n",
    "        raise ValueError(\"invalid adjacency matrix shape\")\n",
    "    \n",
    "    # see how many elements there are in the matrix\n",
    "    mag_vset = 0\n",
    "    cx = adjacency_matrix.tocoo()\n",
    "    for i,j,v in zip(cx.row, cx.col, cx.data): mag_vset += 1\n",
    "    mag_vset *= number_of_queries\n",
    "    \n",
    "    # make the pandas dataframe\n",
    "    df = pd.DataFrame(np.zeros(shape=(mag_vset,11)))\n",
    "    df.columns = [\"Time\",\"Actual Query Time\",\"Origin Latitude\",\"Origin Longitude\",\"Origin Address\",\n",
    "                  \"Destination Latitude\",\"Destination Longitude\",\"Destination Address\",\n",
    "                  \"Distance\",\"Traffic Time\",\"Normal Time\"]\n",
    "    \n",
    "    # go through all elements with the query\n",
    "    df_index = 0\n",
    "    for qi in range(number_of_queries):\n",
    "        time = datetime.datetime.now()\n",
    "        for i,j,v in zip(cx.row, cx.col, cx.data):\n",
    "            # get origin and destination\n",
    "            origin = (location_matrix[:,i][0],location_matrix[:,i][1])\n",
    "            destination = (location_matrix[:,j][0],location_matrix[:,j][1])\n",
    "            # Actually Query GCP (Distance Matrix)\n",
    "            (realtime_travel, time_travel, dist, oa, da) = scrape_gmaps_data(api_key, origin, destination)\n",
    "            # Add all data to dataframe\n",
    "            df.iloc[df_index,0], df.iloc[df_index,1] = time, datetime.datetime.now()\n",
    "            df.iloc[df_index,2],df.iloc[df_index,3],df.iloc[df_index,4] = origin[0], origin[1], oa\n",
    "            df.iloc[df_index,5],df.iloc[df_index,6],df.iloc[df_index,7] = destination[0], destination[1], da\n",
    "            df.iloc[df_index,8], df.iloc[df_index,9], df.iloc[df_index,10] = realtime_travel, time_travel, dist\n",
    "            df_index += 1\n",
    "        # wait to get to whole period before new queries\n",
    "        if qi != number_of_queries - 1:\n",
    "            t.sleep(period * 60 - (datetime.datetime.now() - time).total_seconds())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_matrix = np.array([[33.776360,33.772428],[-84.397824,-84.392709]])\n",
    "adjacency_matrix = lil_matrix((2,2),dtype = '?')\n",
    "adjacency_matrix[0,1] = True\n",
    "adjacency_matrix[1,0] = True\n",
    "\n",
    "df = graph_query(location_matrix,adjacency_matrix,3/60,3,api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 25)\n"
     ]
    }
   ],
   "source": [
    "# center of the mesh\n",
    "center_mesh = [33.776360,-84.397824]\n",
    "# how many rows to make mesh\n",
    "num_rows = 5\n",
    "# how many columns\n",
    "num_columns = 5\n",
    "# distance between them\n",
    "delta = .005\n",
    "lat = []\n",
    "lon = []\n",
    "# make all vertices\n",
    "for i,j in itertools.product(range(num_columns), range(num_rows)):\n",
    "    lat.append(center_mesh[0] + (i-num_rows/2)*delta)\n",
    "    lon.append(center_mesh[1] + (j-num_columns/2)*delta)\n",
    "# magnitude of vector set\n",
    "mag_vset = len(lat)\n",
    "# numpy this boy\n",
    "location_matrix = np.array([lat,lon])\n",
    "# add only the \"mesh\" coordinates (one hop neighbors)\n",
    "adjacency_matrix = lil_matrix((mag_vset,mag_vset),dtype = '?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Take home project: Add all the \"mesh\" edges to the adjacency list in O(N) time####\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Take home project: Implement csv_graph method, which goes from a csv to a graph representation of the query.\n",
    "#### Assume that the dataset was created as above. Please make the adj. mat. sparse (look above for example). The values\n",
    "#### on the edges should not be booleans but instead time magnitude. The default will be traffic time BUT you should support\n",
    "#### using normal time and distance (use that optional param). This should be efficient!!!####\n",
    "\n",
    "def csv_graph(csv, location_matrix, adjacency_matrix, value = \"Traffic Time\"):\n",
    "    return adjacency_matrix\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = graph_query(location_matrix,adjacency_matrix,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.getcwd(), \"..\")\n",
    "path = os.path.join(path, \"dataset\")\n",
    "path = os.path.join(path, \"dataset.csv\")\n",
    "df.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
